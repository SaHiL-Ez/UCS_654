# ğŸ“Š Sampling Techniques on Imbalanced Credit Card Dataset

<div align="center">

### Handling Class Imbalance Using Oversampling (SMOTE)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge&logo=python&logoColor=white)
![Scikit-Learn](https://img.shields.io/badge/scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)
![Status](https://img.shields.io/badge/Status-Completed-success?style=for-the-badge)

</div>

---

## ğŸ” Project Overview

In real-world machine learning applications, datasets are often **highly imbalanced**, where one class significantly dominates the other. This imbalance can severely degrade model performance, especially for critical tasks such as **fraud detection**.

This project focuses on:
- ğŸ“‰ **Analyzing** an imbalanced credit card transaction dataset
- ğŸ“Š **Visualizing** class imbalance
- âš–ï¸ **Applying SMOTE** (Synthetic Minority Oversampling Technique)
- ğŸš€ **Demonstrating** how oversampling improves class distribution and model reliability

---

## ğŸ¯ Objectives

| Goal | Description |
| :--- | :--- |
| **Analyze** | Understand class imbalance in the dataset |
| **Visualize** | Compare class distribution before and after balancing |
| **Balance** | Apply SMOTE oversampling to the dataset |
| **Improve** | Eliminate class bias for better model learning |
| **Present** | Provide clear visual evidence for academic evaluation |

---

## ğŸ“ Dataset Description

| Feature | Details |
| :--- | :--- |
| **Dataset Name** | `Creditcard_data.csv` |
| **Problem Type** | Binary Classification |
| **Target Column** | `Class` |
| **Classification** | `0` â†’ Non-Fraud Transaction <br> `1` â†’ Fraud Transaction |
| **Challenge** | âš ï¸ Extreme imbalance (fraud cases are very rare) |

---

## ğŸ“Š Class Distribution Analysis

Before applying any balancing technique, the dataset is **severely skewed** toward non-fraud transactions. This causes machine learning models to favor the majority class and ignore fraudulent cases.

> **Solution:** To address this issue, **SMOTE oversampling** is applied to synthetically generate minority-class samples.

---

## ğŸ–¼ï¸ Before vs After SMOTE Oversampling

The following visualization clearly illustrates the effect of SMOTE on class distribution:

<p align="center">
  <img src="images/before_after_smote.png" alt="Before and After SMOTE" width="800">
</p>

### ğŸ”¹ Interpretation of the Visualization

| Phase | Non-Fraud (0) | Fraud (1) | Model Impact |
| :--- | :---: | :---: | :--- |
| **Before Balancing** | ğŸ† Dominant | ğŸ“‰ Very few samples | Biased toward Majority |
| **After SMOTE** | âœ… Balanced | âœ… Equal Representation | Fair & Reliable Learning |

---

## ğŸ§  Why SMOTE?

**SMOTE** (Synthetic Minority Oversampling Technique) improves learning by:

- âœ… **Preserving** all majority-class data  
- ğŸ”„ **Generating** synthetic minority samples instead of duplicating data  
- âš–ï¸ **Reducing** model bias  
- ğŸ“ˆ **Improving** generalization and predictive accuracy  

> [!TIP]
> **Why not Undersampling?** 
> SMOTE is preferred over undersampling because it avoids **information loss**.

---

## ğŸ“ˆ Key Insights

- ğŸš¨ **Severe class imbalance** exists in raw credit card data.
- ğŸ“‰ **Visual analysis** confirms minority class under-representation.
- âœ… **SMOTE successfully balances** the dataset (50/50 split).
- ğŸ† **Balanced data** leads to improved model performance and fairness.

---

<div align="center">
  <i>Created for Academic Evaluation & Research</i>
</div>
